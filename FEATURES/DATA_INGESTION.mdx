---
title: "Data Ingestion in Macru"
description: "Learn how Macru connects to various sources and processes your data to build a unified knowledge base."
icon: "database"
---

Macru is designed to centralize your fragmented information by securely ingesting data from various sources. This process makes your data accessible for unified querying and analysis, forming the backbone of your personal knowledge engine.

## Ingestion Process Overview

The journey of your data into Macru involves several key steps:

1.  **Connection**: Establishing a secure link to a data source (e.g., direct file uploads, or services like Notion, Google Calendar, and Gmail through dedicated connectors).
2.  **Data Retrieval**: Fetching or receiving the raw data from the connected source.
3.  **Intelligent Processing**:
    *   Extracting meaningful text content.
    *   Identifying and extracting available structured metadata (e.g., creation dates, authors, task statuses, event times).
    *   Segmenting text into manageable, semantically relevant chunks.
    *   Generating vector embeddings for each chunk to enable powerful semantic search.
4.  **Secure Storage**: Storing all processed data—including original content references, extracted text, chunks, metadata, and embeddings—securely within your Supabase instance.

## Sources of Information

### Direct File Uploads

Easily upload your existing documents directly into Macru.

<Note icon="file-circle-check">
  **Supported Formats Typically Include:**
  *   PDF (`.pdf`)
  *   Microsoft Word (`.docx`)
  *   Plain Text (`.txt`)
</Note>

Uploaded files are seamlessly processed through the standard pipeline: text is extracted, content is chunked, and embeddings are generated and stored, making them immediately available for querying.

<Tip>
  For more technical details on the file upload system, refer to `devlog.txt` entries like `[2025-03-29] File Upload System Implementation` and `[2025-04-04] Enhanced File Management System`.
</Tip>

### Data Connectors

Macru utilizes a modular `DataConnector` pattern (defined in ``lib/types/data-connector.ts``) to integrate with a growing list of external services. This approach ensures Macru can easily adapt to new data sources you might use.

<CardGroup cols={1}> 
  <Card title="Notion Integration" icon="file-lines" href="/CONNECTORS/NOTION_CONNECTOR">
    Connect your Notion workspace via OAuth 2.0. Macru fetches page content and structured properties (Status, Dates, People, etc.), with support for automatic background sync.
    *Implementation details in `devlog.txt`: `[2025-04-24] Notion Connector Setup`, `[2025-04-30] Automatic Notion Sync & Source Timestamps`, `[2025-05-01] Structured Metadata Handling`.*
  </Card>
  <Card title="Google Calendar Integration" icon="calendar-days" href="/CONNECTORS/GOOGLE_CALENDAR_CONNECTOR">
    Link your Google Calendar via OAuth 2.0. Macru ingests event details like title, description, attendees, location, and times. Initial sync focuses on current/future events.
    *Implementation details in `devlog.txt`: `Google Calendar Connector Implementation & Debugging`, `Google Calendar Connector Debugging Session 2`, `[2025-05-03] Google Calendar Connector Refinement`.*
  </Card>
  <Card title="Gmail Integration" icon="envelope" href="/CONNECTORS/GMAIL_CONNECTOR">
    Authorize access to your Gmail account via OAuth 2.0. Macru retrieves email content (subject, body, sender, date). Initial sync typically covers recent emails.
    *Implementation details in `devlog.txt`: `[2025-05-07] Gmail Connector Implementation (Initial)`, `[2025-05-07] Gmail Connector Source Display Fixes`.*
  </Card>
</CardGroup>

## Document Processing Pipeline

Once data is fetched, it undergoes a sophisticated processing pipeline:

#### 1. Text Extraction
   - Service: ``lib/services/text-extractor.ts``
   - Extracts textual content from files (using libraries like `pdf-parse` for PDFs, `mammoth` for DOCX).
   - For connector data, raw text is usually provided directly by the connector (e.g., Notion page content, email body).

#### 2. Chunking
   - Service: ``lib/services/document-chunker.ts``
   - Divides extracted text into smaller segments (chunks). This is vital for fitting content into LLM context windows and for generating effective, focused embeddings.
   - Macru can employ various chunking strategies (e.g., fixed size, by paragraph, semantic).

#### 3. Embedding Generation
   - Service: ``lib/services/embedding-service.ts``
   - Generates vector embeddings for each text chunk using a chosen LLM (e.g., Google Gemini). These embeddings capture the semantic essence of the text, enabling nuanced search.

#### 4. Secure Storage (Supabase Postgres)
   - **``documents`` table**: Stores rich metadata about ingested items (e.g., original source ID, title, type, owner, `source_created_at`, `source_updated_at`, and structured fields like `event_start_time`, `content_status`).
   - **``chunks`` table**: Holds the individual text chunks, each linked to its parent document.
   - **``embeddings`` table**: Stores vector embeddings corresponding to each chunk, leveraging the `pgvector` extension for efficient similarity searches.

<Note>
  Detailed information on the pipeline and database schema can be found in `devlog.txt` entries: `[2025-04-16] Document Processing Pipeline Implementation`, `[2025-04-09] Document Processing Database Schema Setup`, and `[2025-05-01] Structured Metadata Handling (Task 15.1 & 15.2)`.
</Note>

## Structured Metadata Handling

<Tip icon="sparkles">
Beyond free-text, Macru intelligently captures and utilizes structured metadata from your sources. This enriches your data and powers more precise queries.
</Tip>

This includes attributes like:
*   Timestamps (creation, modification, event start/end, due dates)
*   Status fields (e.g., task completion, document approval)
*   Priority levels
*   Participants or Assignees
*   Locations

This structured metadata is stored in the ``documents`` table and plays a crucial role in Macru's Hybrid Query Engine, working alongside vector search to deliver highly relevant and context-aware results. 